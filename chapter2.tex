% chap2.tex (Definitions)



\chapter{Related Work}\label{background}

This section outlines some of the state of the art in the area of data provenance collection systems, data compression techniques for data provenance, and models for representing provenance.


\section{Related Work on Provenance Collection Systems}

There has been a considerable amount of work done on data provenance collection. Some of this work has been focused on databases, sensor networks, scientific workflow systems and file system provenance but so far little attention has been given to provenance in IoT devices. Some the prior work done on data provenance collection are outlined below:

\subsection{Provenance Aware Storage System(PASS)}
MuniswamyReddy
et al developed a provenance aware system that tracks  system level provenance of the linux file system.\textcolor{red}{ There are two versions PASS v1 and PASS v2. v1 allows... while version 2...} Provenance information
is stored in the same location as the file system for easy accessibility, backup,
restoration, and data management. Provenance information is collected and stored in
the kernel space. PASS is composed of 3 major components: provenance collector, and provenance storage, provenance query.The collector keeps track of system level provenance. It intercepts system calls which are translated into provenance data and initially stored in memory as inode catche. This provenance data is then transfered to a file system in an kernel database, BerkleyDB. This database maps key value pairs for provenance data for fast index look up. It also provides functionalities for querying provenance data in the database. The query tool is built ontop of Berkley DB. For querying, users can process queries using the provenance explorer. Query is done using commands such as MAKEFILE  GENERATION which creates the sequence of events that led to the final state of a file or process. DUMP ALL, gets the provenance of the requested file.PASS stores a refrence to the executable that created the provenance data, input files, A description of hardware in which provenance data is produced, OS information, process environment, process parameters, and other/data such as a random number generator seed.PASS detects and eliminates cycles that might occur in provenance dependencies as a result of version avoidance.Cycles violates the dependency relationships between entities. For example, a child node could depend on a parent node and also be an ancestor of a parent node. PASS eliminates cycles by merging processes that might have led to the cycles. 

\subsection{HiFi}
Bates et al. [2] developed system level provenance information for the Linux kernel using Linux Provenance Modules(LPM), which tracks at whole system provenance including interprocess communication, networking, and kernel activities. This is achieved by mediating access to kernel objects. Linux Security Model is a framework that was designed for providing custom access control into the Linux kernel.It consists of a set of hooks which is executed before access decision is made.LSM was designed to avoid problem created by direct system call interception. The provenance information from the kernel space is securely transmitted to the provenance recorder in the user space. 
\par HiFi contains three components, provenance collector, provenance log and provenance handler. The collector and log are contained in the kernel space while the handler is contained in the user space.The log is a storage medium which transmits the provenance data to the user space.The collector contains the LSM which resides the kernel space. The collector records provenance data and writes it to the provenance log.The handler intercepts the provenance record from the log.\textcolor{red}{This approach to collecting provenance data differs from our work since we focus on embedded systems and are concerned with input and output (I/O) data, which primarily involve sensor and actuator readings.}

\subsection{RecProv}

RecProv is a provenance system which records user level provenance, avoiding the overhead incurred by kernel level provenance recording.It uses mozilla rr to perform deterministic record and replay by monitoring system calls  and non deterministic input.Mozilla rr is a debugging tool for linux browser. It is developed for the deterministic recording and replaying of firefox browser in linux. It also ensure the integrity of provenance data up till the point at which a host is compromised by trace isolation. Mozilla rr relies on PTRACE which intercepts system calls during context switch.It uses PTRACE to monitor the CPU state ducring and after a system call. System calls such as execve, clone, fork, open, read, write, clode, dup, mmap, socket, connect, accept are monitored which is used for file versioning. the provenance information generated in converted into PROV-JSON a W3C standard for relaying provenacne information and also stores provenance data in Neo4j a graph database for visualization of provenance graphs and storage.It does not require changes to the kernel like most provenance monitoring systems (include citations of other provenance monitoring systems that require kernel modification). 

Recprov uses PTRACE\_PEEKDATA from PTRACE to access the derefrenced address of the traced process from the registers. 

\subsection{StoryBook}
Spillance et al developed a user space provenance collection system, Storybook \cite{Rabinovich1995}  that allows the collection of provenance data from the user space thereby reducing performance oververhead from kernel space provenance collection. This system is modular.It allows the use of application specific extensions allowing additions such as database provenance, system provenance, and web and email servers. It achieves provenance capture by using FUSE for system level provenance and MYSQL for database level provenance capture. StoryBook allows developers to implemement provenance inspectors. these are custom provenance models which captures the provenance of specific applications which are often modified by different application(e.g web servers, databases). When an operation is performed on a data object, the appropriate provenance model is triggered and provenance data for that data object is captured. SotryBook stores provenance information such as open, close, read or write, application specific provenance, causality relationship between entities contained in the provenance system(?). Provrenance is stored in key value pairs and It uses Fable as the storage backend. Storybook allows for provenance query.It achieves this by looking up inode in the file, ino hashtable.

\subsection{Trustworthy whole system provenance for Linux kernel}

Bates et al provide a security model for provenance collection in the linux kernel. This is achieved by creating a Linux Provenance Model(LPM). LPM serves as a security layer that provides a security abstraction for provenance data. It is involved in the attestation disclosure of the application layer, authentication channel for the network layer.The goal of LPM is to provide an end to end provenance collection system. 

LPM ensures the following security guarantees: for LPM,the system must be able to detect and avoid malicous forgery of provenance data. The system must be trusted and easily verifiable. 

\par When an application invokes a system call, the LPM tracks system level provenance which is transmitted to the Provenance module via the relay buffer . The provenance module registers contains hooks which records system call events. These events are sent to a provenance recorder in the user space. The provenance recorder ensures that the provenance data is sored in the appropriate backend of choice. Proveance recorders offer storage for Gzip, PostGreSQL, Neo4j and SNAP.



\subsection{Towards Automated Collection of Application-Level Data Provenance}
Tariq et all developed a provenance collection system which automatically collects the provenance of applications source code at run time. It takes a different approach from system level provenance capture. It achieves provenance capture by using LLVM compiler framework and SPADE provenance management system. LLVM is a framework that allows dynamic compilation techniques of applications written in C, C++, Objective-C, and Java.Provenance is inserted during compilation.LLVM contains an LLVM Reporter. This is a java class that parses the output file collected from the LLVM Tracer and forwards the provenance data collected to the SPADE Tracer. SPADE is a provenance management tool that allows for the tranformation of domain specific activity into provenance data. The LLVM Tracer module tracks provenance at the exit and entry of each function. The output is an Open Provenance Model in which functions are represented by an OPM process, arguments and return values are represented as an OPM artifact. To minimize provenance record, users are allowed to specify what functions they would like to collect provenance information at run time. The system discards provenance data that are not needed. This does not provide a whole system provenance of all activities that occurs on the system.The LLVM optimizer generates a graph of the target application. This graph is used to perform reverse reachability analysis  from the functions contained in the graph. 


A workflow of how the framework works in collecting provenance is as follows:

\begin{itemize}
\item The application is compiled and converted into bitcode using the LLVM C compiler, clang.

\item The LLVM Tracer module is compiled with the bitcode.

\item Provenance instrumentation is added to the bitcode via the LLVM Trace module.

\item instrumentation bitcode is converted into assembly code via the compiler llc

\item LLVM Tacer and Reporter is compiled into assembly code.

\item  The instrumented application and assembly code is compiled into an executable binary and sent to the SPADE kernel.
\end{itemize}

One major limitation to this approach of collecting application level provenance is that a user is required to have the source code of the specific application in which provenance data is required.Also, provenance collection is limited to function's exit and entry points.


\subsection{Provenance from Log Files: a BigData Problem}


Goshal et al developed a framework for collecting provenance data from log files.They argue that log data contains vital information about a system and can be an important source of proveance information.Provenance is categorized based on two types: process provenance which involves provenance information of the process in which provenance is captured. Data provenance describes the history of data involved in the execution of a process.Provenance is collected by using a rule based approach which consists of a set of rules defined in XML.This framework consists of a Rule Engine. The rule engine processes raw log files to structured provence. There are three categories of rules as specified by the grammar. The match-set rules which selects provenance data based on a set of matching rules. Link Rules which specifies relationship between provenance events and Remap rules are used to specying an alias for a provenance object.


The framework consists of three major components. The Rule engine which contains XML specifications for selecting, linking and remaping provenance objects from log files. This component is integrated with the log processor. The Log processor component is involved in parsing log files with information contained in the rule engine. It converts every matching log information to a provenance graph representing entities(vertices) and their relationship(edges). The adaptor converts the structued provenance generated by the Log processor into serialized XML format which is compatible with karma a provenance service application that is emplyed for the storage and querying of the provenance data collected.




\subsection{Towards a Universal Data Provenance Framework using Dynamic Instrumentnation}

Gessiou et al propose a dyamic unstrumentation framework for data provenance collection that is universal and does not incure the overhead of kernel or source code modifications. This is achieved by using DTrace, a dynamic instumentation utility that allows for the instrumentation of user-level and kernel-level code and with no overhead cost when disabled. It allows to be included at every point in time in a rsystem  runtime without overhead issues from diabled probes.

The goal is to provide an easy extension to add provenance collection to any application regardless of its size or complexity. They implement the provenance collection scheme on file system using PASS, on a database(SQLite) and a web browser(Safari). 
The logging component monitors all system calls for all processes involved. It contains information such as system-call arguments, return value, user id, process name, process if and timestamp. The logging components include functionalities to collect library and functional calls.They argue that applying data provenance to different layers of the software stack and not just the system level can provide varying levels of information.  Provenance needs to be captured at the layer in which it is closest to the information that is needed to be captured. They collect provenance information that pertains to complex systems activities such as a web browser or a database. The information is collected from a process system-call and functional-call based on valid entry points contained in the process. It allows information from multiple entry points. DTrace dynamic instrumentation helps in discovering interesting paths in a system in which data propagates. This helps users use this information to collect a more accurate provenance data.



\subsection{user space provenance with Sandboxing}
\textcolor{red}{TODO}


\subsection{Provenance for Sensors}
Lim et al. developed a
model for calculating the trust of nodes contained in a sensor network by using data
provenance and data similarity as deciding factors to calculate trust. The value of
provenance signifies that the more similar a data value is, the higher the trust score.
Also, the more the provenance of similar values differ, the higher their trust score.
This work differs from our approach since the authors focus on creating a trust score
of nodes connected in a sensor network using data provenance and do not emphasize
how the provenance data is collected. We are focused on creating a secure
provenance aware system for I/O operations which is used to ensure trust of
connected devices.

\subsection{Provenance Recording for Services}
Grouth et al developed a provenance collection system, PReServ that allows software developers to integrate provenance recording into their applications mainly focused on scientific applications. They introduce P-assertion Recording protocol as a way of monitoring process  documentation for Service Oriented Architecture(SOA) the standard for grid systems. PreServ is the implementation of PreP. PreP specifies how provenance can be recorded.  PreServ contains a provenance store web service for recording and querying of p-assertions. P-assertions are provenance data generated by actors about the execution. It contains information about the messages sent and recieved as well as state of the message. PreServ is composed of three layers, the Message Translator which is involved with converting all messages receive via HTTP request to XML format for the provenance store layer. It also determine which plug-in handle to route incoming request, the  Plug-Ins layer receives messages from the Message translator. It implements functionality that the Provenance Store component provides. This component allows third party developers to add new functionality to store provenance data without going through details of the code implementation. The backend component which stores the P-assertions. Various backend implementations could be attached to the system by the plug-in component



\section{Related Works on Data Compression}

\subsection{Secure Data Provenance Compression Using Arithmetic Coding in Wireless Sensor Networks}

The authors encode the provenance of the path in which a sensor provenance travels using arithmetic coding, a data compression algorithm. The WSN contains a base station and a newtork of nodes. The base station is incharge of verifying the integrity of the packets sent. It is also where the arithmetic coding is decoded.The provenance is encoded at the respective nodes and forwarded to the next node in the graph and it is decoded in the base station. 

The BS recovers the characters the same way they were encoded. It also receives the encoded interval which it uses to decode by using the probability values saved in its storage.



For their application,provenance are refered to as the path in which data travels. Provenance refers to where a packet is originated and how it is delivered to the base station. Each node in the WSN is represented as a string of characters and are encoded using arithmetic coding. The base station receives the code and decodes.

Contains two kinds of provenance-Simple provenance in which data are generated from the leaf nodes and are forwarded to surrounding nodes towards the Base Station. In the other form of provenance, data is aggregated at an aggregation node and forwarded to a BS.

Arithmetic coding assigns intervals to a string based  probabilities for the cumulative occurrence of characters contained in the string. It assign probabilities by monitoring the activities of the WSN, this is known as the training phase. In this phase the number of times a node appears in a packet is counted.  Each node in the WSN is regarded as a symbol. This is used as input to the arithmetic coding algorithm which generates a variable interval for the combination of symbols.













%\section{Intrusion Detection for IoT}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
